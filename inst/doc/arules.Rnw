\documentclass[10pt,a4paper]{article}

\usepackage{a4wide}
\setlength{\parskip}{0.5ex plus0.1ex minus0.1ex}
\setlength{\parindent}{0em}

\usepackage[round,longnamesfirst]{natbib}
\usepackage{hyperref}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\textsf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\texttt{#1}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}

\usepackage{Sweave}
%% \VignetteIndexEntry{Data structures for association rules}


\begin{document}
%% ------------------------------------------------------------------
%% ------------------------------------------------------------------
\title{\pkg{arules} -  A Computational Environment for Mining 
    Association Rules and Frequent Item Sets}
\author{Michael Hahsler and Bettina Gr{\"u}n and Kurt Hornik}
\maketitle
\sloppy
%% ------------------------------------------------------------------
%% ------------------------------------------------------------------
\begin{abstract}
  Mining frequent itemsets and association rules is a popular and well
  researched approach for discovering interesting relationships between
  variables in large databases.  The \proglang{R} package \pkg{arules}
  presented in this paper provides a basic infrastructure for creating
  and manipulating input data sets and for analyzing the resulting
  itemsets and rules.  The package also includes interfaces to two fast
  mining algorithms, the popular \proglang{C} implementations of Apriori
  and Eclat by Christian Borgelt.  These algorithms can be used to mine
  frequent itemsets, maximal frequent itemsets, closed frequent itemsets
  and association rules.
\end{abstract}

%% ------------------------------------------------------------------
%% ------------------------------------------------------------------

\section{Introduction}

Mining frequent itemsets and association rules is a popular and well
researched method for discovering interesting relations between
variables in large databases. \cite{arules:Piatetsky-Shapiro:1991}
describes analyzing and presenting strong rules discovered in databases
using different measures of interest.  Based on the concept of strong
rules, \cite{arules:Agrawal+Imielinski+Swami:1993} introduced the
problem of mining association rules from transaction data as follows.

Let $I=\{i_1, i_2,\ldots,i_n\}$ be a set of $n$ binary attributes called
\emph{items}.  Let $\set{D} = \{t_1, t_2, \ldots, t_m\}$ be a set of
transactions called the \emph{database}.  Each transaction
in~$\set{D}$ contains a subset of the items in~$I$.

A \emph{rule} is defined as an implication of the form $X \Rightarrow Y$
where $X, Y \subseteq I$ and $X \cap Y = \emptyset$.  The sets of items
(for short \emph{itemsets}) $X$ and $Y$ are called \emph{antecedent}
(left-hand-side or LHS) and \emph{consequent} (right-hand-side or RHS)
of the rule.

For convenience we introduce 
$\set{X} = \{X_1, X_2, \ldots, X_l\}$
for sets of itemsets with length $l$.
Analogous, we define $\set{R}$ for sets of rules.

To select interesting rules from the set of all possible rules,
constraints on various measures of significance and interest can be
used.  The best-known constraints are minimum thresholds on support and
confidence.  \emph{Support} is defined on an itemset as the proportion
of transactions in the data set which contain the itemset.  All itemsets
which have a support above a set minimum support threshold are called
\emph{frequent itemsets}.  
Finding frequent itemsets can be seen as a simplification of
the unsupervised learning problem called
``mode finding'' or 
``bump hunting''~\citep{arules:Hastie+Tibshirani+Friedman:2001}.
For these problems each item is seen as a variable.
The goal is to find
prototype values so that the probability density
evaluated at these values is sufficiently large.
However, for practical applications with a large number of variables, 
probability estimation will be unreliable and computationally too expensive.
This is why in practice frequent itemsets are used instead of 
probability estimation.

\emph{Confidence} is defined on rules as $\mathrm{conf}(X\Rightarrow Y)
= \mathrm{supp}(X \cup Y) / \mathrm{supp}(X)$.  This can be interpreted
as an estimate of the probability $P(Y|X)$, the probability of finding
the RHS of the rule in transactions under the condition that these
transactions also contain the LHS 
\citep[see e.g.,][]{arules:Hipp+Guentzer+Nakhaeizadeh:2000}.  
Association rules are
required to satisfy both constraints, minimum support and
minimum confidence, at the same time.

At medium to low support values, often a great number of frequent
itemsets are found in a database.  However, since the definition of
support enforces that all subsets of a frequent itemset have to be also
frequent, it is sufficient to only mine all \emph{maximal frequent
  itemsets}, defined as frequent itemsets which are not proper subsets
of any other frequent
itemset~\citep{arules:Zaki+Parthasarathy+Ogihara+Li:1997}.  Another
approach to reduce the number of mined itemsets is to only mine
\emph{frequent closed itemsets.}  An itemset is closed if no proper
superset of the itemset is contained in each transaction in which the
itemset is contained~\citep{arules:Pasquier+Bastide+Taouil+Lakhal:1999,
  arules:Zaki:2004}.  Frequent closed itemsets are a superset of the
maximal frequent itemsets.  Their advantage over maximal frequent
itemsets is that in addition to be able to infer all frequent itemsets,
they also preserve the support information for all frequent itemsets
which can be important for computing additional interest measures after
the mining process is finished (e.g., confidence for rules
generated from the found itemsets, or 
\emph{all-confidence}~\citep{arules:Omiecinski:2003}).


A practical solution to the problem of finding too many 
association rules with support and confidence is
to further filter or rank found rules using additional interest measures.
A popular measure for this purpose is 
\emph{lift}~\citep{arules:Brin+Motwani+Ullman+Tsur:1997}.
Lift is defined on rules as $\mathrm{lift}(X \Rightarrow Y) = 
\frac{\mathrm{supp}(X \cup Y)}{ \mathrm{supp}(X) \mathrm{supp}(Y)}$.
It can be interpreted as the deviation of the support of the whole 
rule from the support expected under independence given the 
supports of the LHS and the RHS.
Greater lift values indicate stronger associations.


In the last decade research on algorithms to solve the frequent itemset
problem has been abundant.  \cite{arules:Goethals+Zaki:2004} compare the
currently fastest algorithms.  Among these algorithms are the
implementations of the Apriori and Eclat algorithms by
\cite{arules:Borgelt:2003} interfaced in the package \pkg{arules}.
The two algorithms use very different mining strategies.  Apriori,
developed by \cite{arules:Agrawal+Srikant:1994}, is a level-wise,
breadth-first algorithm which counts transactions.  In contrast, Eclat
\citep{arules:Zaki+Parthasarathy+Ogihara+Li:1997} employs equivalence
classes, depth-first search and set intersection instead of counting.
The algorithms can be used to mine frequent itemsets, maximal frequent
itemsets and closed frequent itemsets.  The implementation of Apriori
can additionally be used to generate association rules.

The \proglang{R}~\citep{arules:R:2005} 
package \pkg{arules} presented in this paper provides
the infrastructure needed to create and manipulate input data sets for
the mining algorithms and for analyzing the resulting itemsets and
rules.  Since it is common to work with large sets of rules and
itemsets, the package uses sparse matrix representation to minimize
memory usage.  The infrastructure provided by the package was also
created to explicitly facilitate easy extensions, both for interfacing
new algorithms and for adding new types of interest measures and
associations.

The rest of the paper is organized as follows: In the next section, we
give an overview of the data structure implemented in the package
\pkg{arules}.  
%In Sections~\ref{sec:transactions} and \ref{sec:associations} 
In Section~\ref{sec:overview}
we introduce the functionality of the classes to
handle transaction data and associations.  In
Section~\ref{sec:interfaces} we describe the way mining algorithms are
interfaced in \pkg{arules} using the already implemented interfaces for
Apriori and Eclat as examples.  
In Section~\ref{sec:auxiliary} we present some auxiliary methods 
for support counting, rule induction and sampling
available in \pkg{arules}.
We provide several examples in
%Sections~\ref{sec:example-screen} to \ref{sec:example-allconf}.
Section~\ref{sec:examples}.
The
first two examples show typical \proglang{R} sessions for preparing, 
analyzing and
manipulating a transaction data set, and for mining association rules.
The third example demonstrates how \pkg{arules} can be extended to
integrate a new interest measure.  We conclude with a summary of the
features and advantage of the package \pkg{arules} as a computational
environment for mining association rules and frequent itemsets.

%% ------------------------------------------------------------------
%% ------------------------------------------------------------------
\section{Data structure overview\label{sec:overview}}

To enable the user to represent and work with input and output data of
association rule mining algorithms in \proglang{R}, a well thought out
structure is necessary which can deal in an efficient way with large
amounts of sparse binary data.  The \proglang{S4} class structure 
implemented in
the package \pkg{arules} is presented in
Figure~\ref{fig:arules-classes}.

\begin{figure}[tp]
\centering
\includegraphics[width=12cm]{arules-classes}
\caption{UML class diagram of the \pkg{arules}
package.\label{fig:arules-classes}}
\end{figure}

For input data the classes \class{transactions}
and \class{tidLists} (transaction ID lists, an alternative
way to represent transaction data)
are provided.
The output of the mining algorithms comprises the classes
\class{itemsets} and \class{rules} representing a set of itemsets or
a set of rules, respectively.
Both classes directly extend
a common virtual class called \class{associations} which provides a
common interface.
In this structure it is easy to add a new type of associations by adding
a new class that extends \class{associations}.

Items in \class{associations} and \class{transactions} are implemented
by the \class{itemMatrix} class which provides a facade for the sparse
matrix implementation \class{dgCMatrix} from package \pkg{Matrix}
\citep{arules:Bates+Maechler:2005}.

To control the behavior of the mining algorithms, the two classes
\class{ASparameter} and \class{AScontrol} are used.  Since each
algorithm can use additional algorithm-specific parameters, we
implemented for each interfaced algorithm its own set of control
classes.  We used the prefix \class{AP} for Apriori and \class{EC} for
Eclat.  In this way, it is easy to extend the control classes when
interfacing a new algorithm.


\subsection{Set representation\label{sec:setrepresentation}}
From the definition of the association rule mining problem we saw that
transactions are a set $\set{D}$ containing transactions, each
transaction being an itemset.
Very similar are sets of associations. A set of itemsets $\set{X}$ 
contains itemsets and a set of rules $\set{R}$ contains tuples of itemsets,
one for the LHS and on for the RHS of each rule.

\begin{figure}[tp]
\centering
\includegraphics[width=6cm]{itemsetMatrix}
\caption{Example of a set of itemsets represented as a 
    binary incidence matrix.\label{fig:itemsetMatrix}}
\end{figure}

Sets of itemsets (including transactions) 
can be represented as binary incidence matrices
with columns equal to the number of different
items and rows equal to the number of different itemsets. 
The matrix entries represent the presence (1) or absence (0) of an item in a
particular itemset.
An example of a binary incidence matrix is shown in 
Figure~\ref{fig:itemsetMatrix}.

Since a typical frequent itemsets and transaction data 
(e.g., supermarket transactions) only contains a small number of
items compared to the total number of available items, the
binary incidence matrix will
in general be very sparse with many items and a very large number of
rows.  A natural representation for such data is a sparse matrix
format.  For our implementation we chose the \class{dgCMatrix} which is
defined in the \proglang{R} package \pkg{Matrix} 
implemented by~\cite{arules:Bates+Maechler:2005}. The \class{dgCMatrix}
is a compressed, sparse, column-oriented matrix which contains the
indices of the rows unequal to zero, the pointers to the initial indices
of elements in each column and the non-zero elements of the matrix.
Since the package \pkg{Matrix} does not provide efficient subset selection
functionality which directly works on the \class{dgCMatrix} data structure, 
we implemented a suitable function
in \proglang{C} and interfaced it as the subset selection method (\code{[}).
Despite the column orientation of the \class{dgCMatrix}, it is more
convenient to work with incidence matrices which are row-oriented.  This
makes the most important manipulation, e.g., selecting a subset of transactions
from a data set for mining, more comfortable and efficient.  Therefore,
we implemented the class \class{itemMatrix} providing a row-oriented
facade to the \class{dgCMatrix} which stores a transposed incidence
matrix.  At this level also the constraint that the incidence matrix is
binary (and not real valued as the \class{dgCMatrix}) is enforced.
The data structure of the \class{dgCMatrix} class is not intended to 
be directly accessed by the end user of \pkg{arules}. The interfaces of
\class{itemMatrix} can be used without
knowledge of how the internal representation of the data works.
However, if necessary, the \class{dgCMatrix} can be directly accessed 
by developers to add functionality to \pkg{arules} 
(e.g., to develop new types of associations or interest measures or to
efficiently compute a distance matrix between itemsets for clustering).
In this case, the \class{dgCMatrix} should be accessed using the 
coercion mechanism from \class{itemMatrix} to \class{dgCMatrix} with \code{as}.

In addition to the sparse matrix,
\class{itemMatrix} stores item labels (e.g., name
of the items) and handles the necessary mapping between the item label and the
corresponding column number in the incidence matrix.  Optionally,
\class{itemMatrix} can also store additional information on items. For
example, the category hierarchy in a supermarket setting can be stored which
enables the analyst to select only transactions (or as we later see also rules
and itemsets) which contain items from a certain category (e.g., all dairy
products). 

For \class{itemMatrix}, basic matrix operations
including \code{dim} and subset
selection (\code{[}) are implemented.
The first element of \code{dim} and \code{[} represents itemsets or 
transactions and the second elements represents items. For example,
on a transaction data set in variable \code{data} the subset selection 
\samp{data[1:10, 16:20]} selects a matrix containing the 
first 10 transactions and items 16 to 20.

Since \class{itemMatrix} represents sets of itemsets,
a \code{length} method is provided to get the number of
itemsets in the set. Technically, \code{length} returns the number of
rows in the matrix which is equal to the first element returned by \code{dim}.
\pkg{arules} also provides set operations 
including \code{union}, \code{intersect}
and \code{setequal}. These and many other methods are only possible 
if the used \class{itemMatrix} objects 
are compatible, i.e., if matrices have the same number of 
columns and the items are in the same order.

With \code{combine}, several (compatible) \class{itemMatrix} objects
can be combined. \code{duplicated},
\code{unique} and \code{match} are also available.
To get the actual number of items in the itemsets stored in
the \class{itemMatrix}, the \code{size} method is used. 
It returns a vector with
the number of items (ones) for each element in the set 
(row sum in the matrix).  
Obtaining the sizes from the sparse representation is a very efficient 
operation, since it can be calculated directly from the vector of column
pointers in the \class{dgCMatrix}.
For a purchase incidence matrix, \code{size} will produce 
a vector of length of the
number of transactions in the matrix and each element of the vector
contains the number of items in the corresponding transaction.
This information can be used, e.g., 
to select or filter unusually long or short
transactions. 

The \code{itemFrequency} method calculates the frequency for each 
item in a \class{itemMatrix}. Conceptually, the item frequencies
are the column sums of the binary matrix. Technically, 
column sums can be implemented
for sparse representation efficiently by just
tabulating the vector of row numbers of the non-zero elements in
the \class{dgCMatrix}.
Item frequencies can be used for many purposes. 
For example, they are needed to compute interest measures.
\code{itemFrequency} is also used by the method \code{itemFrequencyPlot} 
to produce a bar plot of item count frequencies or support. 
Such a plot gives a quick overview of a set of itemsets 
and shows which are the most important 
items in terms of occurrence frequency.


Coercion from and to \class{matrix} and
\class{list} primitives is provided where names and dimnames are
used as item labels. 
For the coercion form \class{itemMatrix} to \class{list} there 
exist two possibilities.
The usual coercion to \class{list} with the keyword \code{as} results in a
list of vectors of character strings, each containing the item labels of the
items in the corresponding row of the \class{itemMatrix}. The actual
conversion is done by the method \code{LIST} with its default behavior
(argument \code{decode} set to \code{TRUE}).
If \code{LIST} is called with the argument \code{decode} set to \code{FALSE},
the result is a list of integer vectors with column numbers 
for items instead of the
item labels. For many computations it is often useful to work with 
such a list and later use the item column numbers to go back to the 
original \class{itemMatrix} for,
e.g., subsetting columns.
For decoding items later, a \code{decode} method is also available.


Finally, the \code{image} method can be used to produce a
level plot of an \class{itemMatrix} which is useful for quick 
visual inspection.  
For transaction data sets (e.g., point-of-sale data) such a plot can be very
helpful for checking if the data set contains structural changes (e.g.,
items were not offered or out-of-stock during part of the observation
period) or to find abnormal transactions (e.g., transactions which
contain almost all items may point to recording problems).
Spotting such problems in the data can be very helpful for data preparation.



%% ------------------------------------------------------------------
%% ------------------------------------------------------------------

\subsection{Transaction data\label{sec:transactions}}

The main application of association rules is for market basket analysis
where large transaction data sets are mined.  In this setting each
transaction contains the items which were purchased at one visit to a
retail store \citep[see e.g.,][]{arules:Berry+Linoff:1997}.
Transaction data are normally recorded by point-of-sale
scanners and consists of tuples of the form:

\begin{displaymath}
<\emph{transaction ID}, \emph{item ID}, \ldots >
\end{displaymath}

All tuples with the same transaction ID form a single transaction which
contains all the items given by the item IDs in the tuples.  Additional
information denoted by the dots might be available.  For example, the
customer ID might be available via a loyalty program in a supermarket.
Further information on transactions (e.g., time, location), on the items
(e.g., category, price) or on the customer (socio-demographic variables
as age, gender, etc.)  might be available.

\begin{figure}[tp]
\centering
\includegraphics[width=12cm]{transactionMatrix}
\caption{Example of a set of transaction represented in 
    (a) horizontal layout and (b) vertical layout.\label{fig:transactionMatrix}}
\end{figure}

For mining, the transaction data is first transformed into a binary purchase
incidence matrix with columns equal to the number of different items and rows
equal to the number of different transactions.  The matrix entries represent
the presence (1) or absence (0) of an item in a particular transaction.  This
format is often called the \emph{horizontal} database
layout~\citep{arules:Zaki:2000}.  Alternatively, transaction data can be
represented in a \emph{vertical} database layout in the form of
\emph{transaction ID lists}~\citep{arules:Zaki:2000}.  In this format for each
item a list of IDs of the transactions the item is contained in is stored.  An
example of a transaction data set in horizontal and vertical layout is
depicted in Figure~\ref{fig:transactionMatrix}.  Depending on the algorithm,
one of the layouts is used for mining.  In \pkg{arules} both layouts are
implemented as the classes~\class{transactions} and \class{tidLists}.
Similar to \class{transactions} also \class{tidLists} uses a
sparse representation to store its lists. 
Objects of classes~\class{transactions} and \class{tidLists}
can be directly converted into each other by coercion.

The class \class{transactions} directly extends \class{itemMatrix} and inherits
its basic matrix and set functionality (e.g., subset selection, combine,
union).  In addition, \class{transactions} has a slot to store further
information for each transaction in form of a \class{data.frame}.  The slot can
hold arbitrary named vectors with length equal to the number of stored
transactions.  In \pkg{arules} the slot is currently used to store transaction
IDs, however, it can also be used to store user IDs, revenue or profit, or
other information on each transaction.  With this information subsets of
transactions (e.g., only transactions of a certain user or exceeding a set
profit) can be selected.

Objects of class \class{transactions} can be easily created by coercion from
\class{matrix} or \class{list}.  If names or dimnames are available in these
data structures, they are used as item labels or transaction IDs accordingly.
To import data from a file, the \code{read.transactions} function is provided.
This function reads files structured as shown above and also the very common
format with one line per transaction and the items separated by a predefined
character.  Finally, the method \code{inspect} can be used to inspect
transactions (e.g., an interesting transaction selected with subset selection).

Another important application of mining association rules has been proposed by
\cite{arules:Piatetsky-Shapiro:1991} and \cite{arules:Srikant+Agrawal:1996} for
discovering interesting relationships between the values of categorical and
quantitative (metric) attributes.  For mining associations rules, non-binary
attributes have to be mapped to binary attributes. The straight forward mapping
method is to transform the metric attributes into $k$ ordinal attributes by
building categories (e.g., an attribute income might be transformed into a
ordinal attribute with the three categories: ``low'', ``medium'' and ``high'').
Then, in a second step, each categorical attribute with $k$ categories is
represented by $k$ binary dummy attributes which correspond to the items used
for mining.  An example application using questionnaire data can be found in
\cite{arules:Hastie+Tibshirani+Friedman:2001}.

The typical representation for data with categorical and quantitative
attributes in \proglang{R} is a \class{data.frame}.  First, a domain expert has
to create useful categories for all metric attributes.  This task is
supported in \proglang{R} by functions as \code{cut(x, \ldots)}, etc.  The
second step, the generation of binary dummy items, is automated in package
\pkg{arules} by the \code{coerce} method from \class{data.frame} to
\class{transactions}.  In this process, the original attribute names and
categories are preserved as additional item information and can be used
to select itemsets or rules which contain items referring to a certain original
attributes.  The resulting \class{transactions} object can be mined and
analyzed the same way as market basket data, see the example in
Section~\ref{sec:example-screen}.





%% ------------------------------------------------------------------
%% ------------------------------------------------------------------
\subsection{Associations: itemsets and sets of rules\label{sec:associations}}

The result of mining transaction data in \pkg{arules} are
\class{associations.} 
Conceptually, associations are sets of 
objects denoted by $\set{X} = \{X_1, X_2, \ldots, X_l\}$. 
Each object in the set describes the relationship between some items (e.g., 
as an itemset or a rule) and
has values for different measures of quality assigned.
Such quality measures can be measures of significance (e.g., support) 
or measures of interest (e.g., confidence, lift)
or other measures (e.g., revenue covered by the association).

All types of association have a common interface in \pkg{arules} comprising
the following methods:
\begin{itemize}
\item A \code{summary} method to produces a short overview of the set and
\code{inspect} method to display individual associations,
\item a \code{length} method for getting the number of elements in the set,
\item sorting the set using the values of different quality measures
	(\code{SORT}), 
\item subset extraction (\code{[} and the \code{subset} method),
\item set operations (\code{union}, \code{intersect} and \code{setequal})
\item handling of duplicated elements and matching
	(\code{duplicated}, \code{unique} and \code{match}).
\end{itemize}

Currently implemented association in \pkg{arules} are
sets of itemsets (e.g., used for frequent itemsets of their 
closed or maximal subset) and
sets of rules (e.g., association rules).
Both classes, \class{itemsets} and \class{rules}, directly extend the
virtual class \class{associations} and provide the interface described above.

Class \class{itemsets} contains one \class{itemMatrix} object to store
the items as a binary matrix where each row in the matrix represents an
itemset. In addition, it may contain
transaction ID lists as an object of class \class{tidLists}. 
Note that when representing transactions, \class{tidLists}
stores for each item a transaction list, but here
it stores for each itemset a list of transaction IDs in which the itemset
appears. Such lists are currently only returned by \code{eclat}.

Class \class{rules} consists of two \class{itemMatrix} objects
representing the left-hand-side (LHS) and the right-hand-side (RHS) of
the rules, respectively.  

The items in the associations and the
quality measures can be accessed
and manipulated in a safe way using accessor and replace methods for
\code{quality}, \code{items}, \code{lhs} and \code{rhs}.  In addition
the association classes have built-in validity checking which ensures
that all elements have a matching dimension.

It is simple to add new quality measures to existing associations.
Since the \code{quality} slot holds a \class{data.frame}, additional
columns with new quality measures can be added.  These new measures can
then be used to sort or select associations using the \code{SORT} or the
\code{subset} methods.  Adding a new type of associations to
\pkg{arules} is easy as well.  One has only to implement a new class
extending the virtual \class{associations} class and provide the
interface described above.

%% ------------------------------------------------------------------
%% ------------------------------------------------------------------

\section{Mining algorithm interfaces\label{sec:interfaces}}

In package \pkg{arules} we interface free reference implementations of
Apriori and Eclat by Christian Borgelt
\citep{arules:Borgelt+Kruse:2002,arules:Borgelt:2003}.  The code is
called directly from \proglang{R} by the functions \code{apriori} and
\code{eclat} and the data objects are directly passed from \proglang{R} to
the \proglang{C} code and back without writing to external files.
The implementations can mine association rules, frequent itemsets,
and closed and maximal frequent itemsets. 

The input format of the data for the \code{apriori} and
\code{eclat} functions is
\class{transactions} or a data format which can be coerced to
\class{transactions} (e.g., \class{matrix} or \class{list}).
The algorithm parameters are divided into two groups represented
by the arguments \code{parameter} and \code{control}.
The mining parameters (\code{parameter}) change the
characteristics of the mined itemsets or rules  (e.g., the
minimum support) and the control parameters (\code{control})
influence the
performance of the algorithm (e.g., an initial sorting of the items
with respect to their frequency).
These arguments have to be instances of the
classes \class{APparameter} and \class{APcontrol} for
the function \code{apriori}
or \class{ECparameter} and \class{ECcontrol} for
the function \code{eclat}, respectively.  Alternatively, data which can be coerced to
these classes (e.g., \code{NULL}
which will give the default values or a named list
with names equal to slot names to change the default values) can be passed.
In these classes, each slot specifies a different parameter and the values.  The
default values are equal to the defaults of the stand-alone \proglang{C}
programs \citep{arules:Borgelt:2004} except that by
default the more common original
support definition (instead of the
support of only the antecedent)
is used for the specified minimum support required.

For \code{apriori} the appearance feature implemented by Christian
Borgelt can also be used.  With argument \code{appearance} of function
\code{apriori} one can specify which items have to or cannot appear in
itemsets or rules.  For more information on this feature we refer to the
Apriori manual~\citep{arules:Borgelt:2004}.

The output of the functions \code{apriori} and \code{eclat} is an object
of a class extending \class{associations} which contains the sets of mined
associations and can be further analyzed using the methods provided for
these classes.

It is straightforward to interface additional algorithms which use an
incidence matrix or transaction ID list representation as input.  The
necessary steps are:

\begin{enumerate}
 \item Adding interface code to the algorithm, preferably by directly
  calling into the native implementation language (rather than using
  files for communication), and an \proglang{R} function calling this
  interface.
 \item Implementing extensions for \class{parameter} and
  \class{control}.
\end{enumerate}

There exist many different algorithms which solve the frequent 
and closed frequent itemset problems.
Each algorithm has specific strengths which can be important for
very large databases.
Such algorithms, e.g. kDCI, LCM, FP-Growth or Patricia, are
discussed in~\cite{arules:Goethals+Zaki:2003}.  
The source code is available on the internet and can be interfaced 
in the future for \pkg{arules}. 


\section{Auxiliary methods \label{sec:auxiliary}}

In \pkg{arules} several helpful methods are implemented for
support counting, rule induction, sampling, etc.
In the following we will discuss some of these methods.

\subsection{Counting support for itemsets\label{sec:counting}}
Normally, itemset support is counted during mining the database
with a set minimum support.
During this process all frequent itemsets plus some infrequent candidate
itemsets are counted (or support is determined by other means.
This procedure might take 
some time, especially for low minimum support values. 

If only the support information for a single or a few itemsets is needed,
we might not want to mine the database for all frequent itemsets. 
We also do not know in advance how high (or low) to set the minimum support to
still get the support information for the itemset in question.

For this problem, \pkg{arules} contains the method \code{support}
which determines the support for a set of given sets of items
(as an \class{itemMatrix}) by means of transaction ID set 
intersection (using \class{tidLists}).
Transaction ID set intersection is used by several fast mining algorithms 
(e.g., by Eclat~\citep{arules:Zaki+Parthasarathy+Ogihara+Li:1997}). 
The support of an itemset is determined by intersecting
the transaction ID sets of its subsets. In the simplest case 
the transaction IDs for all items which occur in an itemset are intersected.
The support count is then the size of the intersection set.

In addition to determining the support of a few itemsets without mining all
frequent itemsets, \code{support} is also useful for finding 
the support of infrequent itemsets with a support so low that mining
is infeasible due to combinatorial explosion.


\subsection{Rule induction\label{sec:induction}}
A part of the association rule mining problem is the generation (or induction)
of a set of rules~$\set{R}$ from a set of frequent itemsets~$\set{X}$. 
The implementation of the
Apriori algorithm used in \pkg{arules} already contains a rule induction engine
and returns per default the set of association rules 
of the form $X \Rightarrow Y$ which satisfy given minimum support and
minimum confidence.
Following the definition of \cite{arules:Agrawal+Imielinski+Swami:1993} 
$Y$ is restricted to single items. 

In some cases it is necessary to separate mining itemsets and 
generating rules from itemsets. For example, if only rules stemming 
from a subset of all frequent itemsets are interesting for the user.
The Apriori implementation efficiently generates rules by 
reusing the data structures built during mining the frequent itemsets.
However, if Apriori is used to only return itemsets or Eclat or
some other algorithm is used to mine itemsets, 
the data structure needed for rule induction is lost.
The complete set of frequent itemsets contains all information needed to
induce association rules, since the support information for all
frequent itemsets and its subsets (which are also frequent) is available.
However, for efficient induction, a suitable data structure which allows fast 
look-ups of support values, has to be rebuilt. 

If rules need to be induced from an arbitrary set of itemsets, 
not all needed support information to calculate
confidence is available.
For example, if all available information is 
a itemset containing  five items and we want to induce rules,
we need the support of the itemset (which we might know), but 
also the support of
all subsets of length four.
The missing support information has to be counted from the 
database. If we want to induce rules efficiently for 
a given set of itemsets, we also have to store support values in
a suitable data structure to avoid counting itemsets more than once.

The method \code{ruleInduction} provided in \pkg{arules}
reuses the 
counting mechanism, the data structure of the 
rule induction engine of the 
\proglang{C} implementation of Apriori to induce
rules for a given confidence from an arbitrary
set of itemsets $\set{X}$ in 
the following way:
\begin{enumerate}
\item Reduce the database to only the items which occur in $\set{X}$,
\item determine the lowest support of an itemset in $\set{X}$,
\item reuse the implementation of Apriori to
    mine the set of all rules $\set{R}$ from the reduced database 
    using the given confidence 
    and the lowest itemset support found above,
\item remove the rules from $\set{R}$ which can not be generated from the 
    itemsets in $\set{X}$.
\end{enumerate}

Most time is spent on the rule filtering step since here 
the set of items in each mined rule in $\set{R}$ has to be matched 
against the items in each itemset in $\set{X}$. 
However, if only a small number of distinct items occurs in $\set{X}$,
\code{ruleInduction} is reasonably fast.

Alternatively, the appearance of items in rules can be directly controlled
with the argument \code{appearance} for 
Apriori~\citep[see][]{arules:Borgelt:2004} which might be faster in some cases. 


%\subsection{Other interest measures}

\subsection{Sampling from transactions\label{sec:sample}}
Taking samples from large databases for mining is a
powerful technique. This technique is especially useful if the 
original database does not fit into main memory, but the sample does.
However, even if the database fists into main memory, sampling can provide
an enormous speed-up for mining at the cost of only little 
degradation of accuracy.

\cite{arules:Mannila+Toivonen+Verkamo:1994}
proposed sampling with replacement for association rule mining
and quantify the estimation error due to sampling.
Using Chernov bounds on the binomial distribution (the number of
transactions which contains a given itemset in a sample),
the authors argue that in theory even relatively small samples 
should provide good estimates for support.

\cite{arules:Zaki+Parthasarathy+Li+Ogihara:1997} 
built upon the theoretic work by \cite{arules:Mannila+Toivonen+Verkamo:1994}
and show that for an itemset $X$ 
with support $\tau = \mathrm{supp}(X)$
and for an acceptable relative error of support $\epsilon$
(an accuracy of $1 - \epsilon$)
at a given confidence level $1-c$,
the needed sample size $n$ can 
be computed by

\begin{equation}
n = \frac{-2\mathrm{ln}(c)}{\tau\epsilon^2}.
\label{equ:samplesize}
\end{equation}

Depending on its support, for each 
itemset a different sample size
is appropriate.  
As a heuristic, the authors suggest to use the user specified minimum
support threshold for $\tau$.
This means that for itemsets close to minimum support, the given
error and confidence level hold while for more frequent itemsets the
error rate will be less. However, with this heuristic the error rate for
itemsets below minimum support can exceed $\epsilon$ at the given 
confidence level and thus some infrequent itemset might appear 
as frequent in the sample.

\cite{arules:Zaki+Parthasarathy+Li+Ogihara:1997} 
also evaluated 
sampling in practice on several data sets and conclude that 
sampling not only speeds mining up considerably, but also
the errors are considerably smaller than theoretically obtained by 
Chernov bounds and thus samples smaller than obtained by
formula~\ref{equ:samplesize} are often sufficient.

Another way to obtain the required sample size 
for association rule mining is progressive 
sampling~\citep{arules:Parthasarathy:2002}.
This approach starts with a small sample and uses progressively larger 
samples until model accuracy does not improve significantly anymore. 
\cite{arules:Parthasarathy:2002} defines a proxy for 
model accuracy improvement by using a similarity measure between two
sets of associations. The idea is that 
since larger samples will produce more accurate results, the
similarity between two sets of associations of two consecutive samples
is low if accuracy improvements are high and increases with
decreasing accuracy improvements.
Thus increasing sample size can be stopped if the similarity
between consecutive samples reaches a ``plateau.'' 

\cite{arules:Toivonen:1996} presents an application of sampling to reduce
the needed I/O overhead for very large databases which do not fit into 
main memory.
The idea is to use a random sample from the data base to mine
frequent itemsets at a support threshold below the set minimum support.
The support of these itemsets is then counted in the whole database 
and the infrequent itemsets are discarded. 
If the support threshold to mine the sample is picked low enough, 
almost all frequent itemsets and their support will be found
in one pass over the large database.

In \pkg{arules} sampling is implemented by the method \code{sample}
which provides all capabilities of the standard sampling method in \proglang{R}
(e.g., sampling with or without replacement and probability weights).


%\subsection{Random transactions}
%The function \code{random.transactions} can be used to create a random 
%transaction data set of given size and with given marginal probabilities 
%for the items.
%Each transaction is the result of one independent Bernoulli trial for each 
%item resulting in a data set 
%which only contains noise and no structure.
%Such data sets can be
%used to test how well interest measures are able to suppress noise.

%% ------------------------------------------------------------------
%% ------------------------------------------------------------------

\section{Examples\label{sec:examples}}
In this section we show the basic functionality of arules with some examples.

\subsection{Example 1: Analyzing and preparing a transaction 
data set \label{sec:example-screen}}

In this example, 
we show how a data set can be analyzed and manipulated
before associations are mined.
This is important for finding problems in the data set which 
could make the mined associations useless or at least inferior
to associations mined on a properly prepared data set.
For the example,
we look at the \code{Epub} transaction data contained in
package \pkg{arules}.  This data set contains downloads of documents
from the Electronic Publication platform of the Vienna University of
Economics and Business Administration available via
\url{http://epub.wu-wien.ac.at} from January 2003 to August 2005.

First, we load \pkg{arules} and the data set.

<<>>=
library("arules")
@

<<epub1>>=
data("Epub")
Epub
@

We see that the data set consists of \Sexpr{length(Epub)} transactions
and is represented as a sparse matrix with \Sexpr{length(Epub)} rows
and \Sexpr{dim(Epub)[2]} columns which represent the items. 
Next, we use the summary method to get more information about the 
data set.

<<epub2>>=
summary(Epub)
@

The summary method displays the most frequent items in the data set,
information about the transaction length distribution and that the data
set contains some extended transaction information.  We see that the
data set contains transaction IDs and in addition time stamps (using class
\class{POSIXct}) for the
transactions.  The additional information can be used for analyzing the
data set.

<<>>=
year <- strftime(as.POSIXlt(transactionInfo(Epub)[["TimeStamp"]]), "%Y")
table(year)
@

We selected only the year part of the time stamps. For 2003, the first year
in the data set we have \Sexpr{table(year)[1]} transactions.
We can select the corresponding transactions and inspect the structure
using a level-plot. 

%%% lattice image returnes an object of class "trellis"
<<fig=FALSE>>=
Epub_2003 <- Epub[year == "2003"]
length(Epub_2003)
image(Epub_2003)
@
<<echo=FALSE, fig=TRUE>>=
print(image(Epub_2003))
@

The plot is a direct visualization of the binary incidence matrix where
the the dark dots represent the ones in the matrix.  From the plot we
see that the items in the data set are not evenly distributed.  In fact,
the white area to the top right side suggests, that in the beginning of
2003 only very few items were available (less than 50) and then during
the year more items were added until it reached a number of around 300
items. Also, we can see that there are two transactions in the data set
which contain a very high number of items (denser horizontal lines).
These transactions need further investigation since they could originate
from data collection problems (e.g., a web robot downloading many
documents from the publication site).  To find the very long
transactions we can use the size method and select very long
transactions (containing more than 20 items).

<<>>=
transactionInfo(Epub_2003[size(Epub_2003) > 20])
@

We found three long transactions and printed the corresponding
transaction information. Of course, size can also be used in a similar
fashion to remove long or short transactions.

Transactions can be inspected using the inspect method. 
Since the long transactions identified above would result in
a very long printout, we will inspect 
the first 5 transactions in the subset for 2003.

<<>>=
inspect(Epub_2003[1:5])
@

Most transactions contain one item. Only transaction 4 contains three items. 
For further inspection transactions can be converted into a list with:

<<>>=
as(Epub_2003[1:5], "list")
@

Finally, transaction data in horizontal layout can be converted to
transaction ID list in vertical layout using coercion.

<<>>=
Epub_tidLists <- as(Epub, "tidLists")
Epub_tidLists
@

For performance reasons the transaction ID list
is also stored in a sparse matrix. To get a list, coercion to \class{list}
can be used.

<<>>=
as(Epub_tidLists[1:3], "list") 
@

In this representation each item has an entry
which is a vector of all transactions it occurs in.
Transaction ID list can be directly used as input for mining algorithms which 
use such a vertical database layout to mine associations.

In the next example, we will see how a data set is created and
rules are mined.

\subsection{Example 2: Preparing and mining a 
questionnaire data set\label{sec:example-adult}}

As a second example, 
we prepare and mine questionnaire data.
We use the Adult data set from the UCI machine
learning repository \citep{arules:Blake+Merz:1998} provided by
package~\pkg{arules}.  This data set is similar to the data used by
\cite{arules:Hastie+Tibshirani+Friedman:2001}.  The data originates from
the U.S. census bureau database and contains 48842 instances with 14
attributes like age, work class, education, etc. 
In the original applications of the data, the
attributes were used to predict the income level of individuals.
We added the attribute income with levels \code{small} and \code{large},
representing an income of $\le \$50,000$ and $> \$50,000$, respectively.
This data is included in \pkg{arules} 
as the data set \code{Adult}.


<<data>>=
data("Adult")
dim(Adult)
Adult[1:2,]
@


\code{Adult} contains a mixture of categorical and metric attributes and
needs some preparations before it can be transformed into
transaction data suitable for association mining.
First, we remove the two attributes \code{fnlwgt} and
\code{education-num}. The first attribute is a weight calculated
by the creators of the data set from control data provided by
the Population Division of the U.S. census bureau. 
The second removed attribute is just a numeric representation of the
attribute \code{education} which is also part of the data set.

<<>>=
Adult[["fnlwgt"]] <- NULL
Adult[["education-num"]] <- NULL
@

Next, we need to map the four remaining metric attributes (\code{age},
\code{hours-per-week}, \code{capital-gain} and \code{capital-loss}) to ordinal
attributes by building suitable categories.
We divide the attributes \code{age} and \code{hours-per-week}
into suitable categories using knowledge about typical age groups 
and working hours. 
For the two capital related attributes,
we create a category called \code{None} 
for cases which have no gains/losses. 
Then we further divide the group with gains/losses
at their median into the two categories \code{Low} and \code{High}.


<<>>=
Adult[[ "age"]] <- ordered(cut(Adult[[ "age"]], c(15,25,45,65,100)),
    labels = c("Young", "Middle-aged", "Senior", "Old"))

Adult[[ "hours-per-week"]] <- ordered(cut(Adult[[ "hours-per-week"]],
      c(0,25,40,60,168)),
    labels = c("Part-time", "Full-time", "Over-time", "Workaholic"))
			    
Adult[[ "capital-gain"]] <- ordered(cut(Adult[[ "capital-gain"]],
      c(-Inf,0,median(Adult[[ "capital-gain"]][Adult[[ "capital-gain"]]>0]),1000000)),
    labels = c("None", "Low", "High"))

Adult[[ "capital-loss"]] <- ordered(cut(Adult[[ "capital-loss"]],
      c(-Inf,0,
	median(Adult[[ "capital-loss"]][Adult[[ "capital-loss"]]>0]),1000000)),
    labels = c("none", "low", "high"))
@

Now, the data can be automatically recoded as
a binary incidence matrix by coercing the data set to
\class{transactions}.

<<coerce>>=
Adult_transactions <- as(Adult, "transactions")
Adult_transactions
@

The remaining \Sexpr{dim(Adult)[2]} categorical attributes were
automatically recoded into \Sexpr{dim(Adult_transactions)[2]}
binary items. During encoding the item labels were generated in the
form of 
\texttt{<\emph{variable name}> = <\emph{category label}>}.

<<summary>>=
summary(Adult_transactions)
@

The summary of the transaction data set gives a rough overview showing
the most frequent items, the length distribution of the transactions and
the extended item information which shows which variable and which value
were used to create each binary item. In the first example we see that
the item with label \texttt{age = middle-aged} was generated by variable
\texttt{age} and level \texttt{middle-aged}.  

To see which items are important in the data set we can use the 
\code{itemFrequencyPlot}. To reduce the number of items, we only plot
the item frequency for items with a support greater than 10\%.

<<fig=TRUE, label=itemFrequencyPlot>>=
itemFrequencyPlot(Adult_transactions[, itemFrequency(Adult_transactions) > 0.1])
@

Next, we call the function
\code{apriori} to find all rules (the default association type for
\code{apriori}) with a minimum support of 0.5\% and a confidence of 0.7.

<<apriori>>=
rules <- apriori(Adult_transactions, 
                 parameter = list(support = 0.005, confidence = 0.7))
rules
@

%The specified parameter values are validated and, for example,
%a support $> 1$ gives:
%
%<<error>>=
%error <- try(apriori(Adult_transactions, parameter = list(support = 1.3)))
%error
%@

First, the function prints the used parameters.  Apart from the
specified minimum support and minimum confidence, all parameters have
the default values. It is important to note that with parameter
\code{maxlen}, the maximum size of mined frequent itemsets, is by
default restricted to 5.  Longer association rules are only mined if
\code{maxlen} is set to a higher value.  After the parameter settings,
the output of the \proglang{C} implementation of the algorithm with timing
information is displayed.

The result of the mining algorithm is a set of \Sexpr{length(rules)}
rules.  For an overview of the mined rules the method \code{summary}
can be used.  It shows the number of rules, the most frequent items
contained in the left-hand-side and the right-hand-side and their
respective length distributions and summary statistics for the quality
measures returned by the mining algorithm.

<<summary>>=
summary(rules)
@

As typical for association rule mining, the number of found rules is
huge.  To analyze these rules, for example, the method \code{subset}
can be used to produce a subset of rules which contain items which
resulted form the variable \code{income} in the right-hand-side of the
rule and the \code{lift} measure exceeds $1.4$.

<<rules>>=
rules.sub <- subset(rules, subset = rhs %in% "income" & lift > 1.4)
@

We can then inspect the three rules with the highest lift value (using the
\code{SORT} method).

{\samepage\small
<<subset>>=
inspect(SORT(rules.sub, by = "lift")[1 : 3])
@
}

Using such subset selection and sorting a set of associations can be
analyzed even if it is huge.

\subsection{Example 3: Extending arules with a new interest measure\label{sec:example-allconf}}

In this example, we show how easy it is to add a new interest measure.
As the interest measure we chose \emph{all-confidence}
introduced by \cite{arules:Omiecinski:2003}. All-confidence is
defined on itemsets $X$ as:

\begin{equation}
\mbox{all-confidence}(X) = \frac{\mathrm{supp}(X)}
{\mathrm{max}(\mathrm{supp}(I \subset X))}
\label{equ:all_conf}
\end{equation}

This measure has the property $\mathrm{conf}(I \Rightarrow Z \setminus
I) \ge \mbox{all-confidence}(X)$ for all $I \subset X$.  This means that
all possible rules generated from itemset $X$ must at least have a
confidence given by the itemset's all-confidence value.
\cite{arules:Omiecinski:2003} shows that the support in the denominator
of equation~\ref{equ:all_conf} must stem from a single item and thus can
be simplified to $\max(\mathrm{supp}(i \in X))$.

To obtain an itemset to calculate all-confidence for, 
we mine frequent itemsets from the previously used
Adult data set using the Eclat algorithm.

<<>>=
data("Adult_transactions")
fsets <- eclat(Adult_transactions, parameter = list(support = 0.05), 
	control = list(verbose=FALSE))
@

For the denominator of all-confidence we need to find all mined single
items and their corresponding support values. In the following we 
create a named vector where the names are the column numbers of the 
items and the values are their support.

<<>>=
single_items <- fsets[size(items(fsets)) == 1]

# get the col numbers we have support for
single_support <- quality(single_items)$support
names(single_support) <- unlist(LIST(items(single_items),
	    decode = FALSE))
head(single_support, n = 5)
@

Next, we can calculate the all-confidence 
using formula~\ref{equ:all_conf} for all itemsets.
The single item support needed for the denomination is looked up from the
named vector \code{single\_support} 
and the resulting measure is added to the set's quality data frame.

<<>>=
itemset_list <- LIST(items(fsets), decode = FALSE)

all_conf <- quality(fsets)$support / 
    sapply(itemset_list, function(x) 
    max(single_support[as.character(x)]))

quality(fsets) <- cbind(quality(fsets), all_conf)
@

The new quality measure is now part of the set of itemsets.
<<>>=
summary(fsets)
@

It can be used to
manipulate the set. For example, we can look at the itemsets which contain an
item related to education and sort them by all-confidence (we filter itemsets
of length 1 first, since they have per definition a all-confidence of~1).

<<>>=
fsets.sub <- subset(fsets, subset = items %in% "education")
inspect(SORT(fsets.sub[size(fsets.sub)>1], by = "all_conf")[1 : 3])
@

All-confidence is implemented in \pkg{arules} as the method 
\code{all\_confidence}.

\subsection{Example 4: Sampling}
In this example, we show how sampling
can be used in \pkg{arules}. We use again the Adult data set.

<<>>=
data("Adult_transactions")
Adult_transactions
@

To calculate a reasonable sample size $n$, we use the formula developed
by \cite{arules:Zaki+Parthasarathy+Li+Ogihara:1997} and presented in 
Section~\ref{sec:sample}. We choose a minimum support of 5\%.
As an acceptable error rate for support $\epsilon$ of we choose 10\% and
as the confidence level ($1-c$) we choose 90\%. 

<<>>=
supp <- 0.05
epsilon <- 0.1
c <- 0.1

n <- -2 * log(c)/ (supp * epsilon^2)
n
@

The resulting sample size is considerably smaller than the original data base.
With \code{sample} we produce a sample of size $n$ with replacement from
the database.


<<>>=
Adult.sample <- sample(Adult_transactions, n, replace = TRUE)
@

The resulting sample can be compared with the 
database (the population) using an item frequency plot.
The item frequencies in the sample are displayed as bars and the
item frequencies in the original database are represented by 
the line. For better readability of the labels, we
only display frequent items in the plot and reduce
the label size with the parameter \code{cex.names}.

<<fig=TRUE>>=
items.frequent <- itemFrequency(Adult_transactions) > supp
itemFrequencyPlot(Adult.sample[,items.frequent],
    population = Adult_transactions[,items.frequent],
    cex.names = 0.8)
@

To compare the speed-up reached by sampling we use the Eclat algorithm
to mine frequent itemsets on both, the database and the sample
and compare the system time (in seconds) used for mining.

<<>>=
t <- system.time(itemsets <- eclat(Adult_transactions, 
    parameter = list(support = supp), control = list(verbose = FALSE)))
t

t.sample <- system.time(itemsets.sample <- eclat(Adult.sample, 
    parameter = list(support = supp), control = list(verbose = FALSE)))
t.sample
@


Mining the sample instead of the whole data base results in a speed-up
factor of:
<<>>=
# speed up
t[1] / t.sample[1]
@


To evaluate the accuracy for the itemsets mined from the sample, we
analyze the difference between the two sets.

<<>>=
itemsets
itemsets.sample
@

The two sets have roughly the same size. To check if the sets contain
similar itemsets, we match the sets and see what fraction of
frequent itemsets found in the database were also found in the sample. 


<<>>=
match <- match(itemsets, itemsets.sample)
## remove no matches
#match <- match[!is.na(match)]
length(match[!is.na(match)]) / length(itemsets)
@

Almost all frequent itemsets were found using the sample.
The summaries of the support of the frequent itemsets 
which were not found in the sample and the itemsets
which were frequent in the sample although they
were infrequent in the database give:

<<>>=
summary(quality(itemsets[which(is.na(match))])$support)
summary(quality(itemsets.sample[-match[!is.na(match)]])$support)
@

This shows that only itemsets with support very close to the minimum support
were falsely missed or found.

For the frequent itemsets which were found in the database and in the
sample, we can calculate accuracy from the the error rate.

<<>>=
#accuracy <- 1 - abs(quality(itemsets[!is.na(match)])$support - 
#    quality(itemsets.sample[match[!is.na(match)]])$support) / 
#    quality(itemsets[!is.na(match)])$support

supp.itemsets <- quality(itemsets[!is.na(match)])$support
supp.itemsets.sample <- quality(itemsets.sample[match[!is.na(match)]])$support

accuracy <- 1 - abs(supp.itemsets.sample - supp.itemsets) / supp.itemsets

summary(accuracy)
@

The summary shows that sampling resulted in finding the support of itemsets
with high accuracy. Thus for extremely large databases or
for application where mining time is important, sampling provides
a powerful technique.


%% ------------------------------------------------------------------
%% ------------------------------------------------------------------

\section{Summary and outlook\label{sec:conclusion}}

With package \pkg{arules} we
provide the basic infrastructure which enables us to 
mine associations and analyze and manipulate the results. 
%easily combine
%association mining with clustering and visualization techniques already
%available in \proglang{R}.  
Previously, in \proglang{R} there was no such infrastructure available.
The main features of \pkg{arules} are:

\begin{itemize}
 \item Efficient implementation using sparse matrices.
 \item Simple and intuitive interface to manipulate and analyze
  transaction data, sets of itemsets and rules with subset selection and
  sorting.
 \item Interface to two fast mining algorithms.
 \item Flexibility in terms of adding new quality measures, and
  additional item and transaction descriptions which can be used for
  selecting transactions and analyzing resulting associations.
 \item Extensible data structure to allow for easy implementation of new
  types of associations and interfacing new algorithms.
\end{itemize}

There are several interesting possibilities to extend \pkg{arules}.  For
example, it would be very useful to interface algorithms which use
statistical measures to find ``interesting'' itemsets (which are not
necessarily frequent itemsets as used in an association rule context).
Such algorithms include implementations of the $\chi^2$-test based
algorithm by \cite{arules:Silverstein+Brin+Motwani:1998} or the baseline
frequency approach by \cite{arules:DuMouchel+Pregibon2001}.

Another interesting extension would be to interface synthetic data 
generators for fast evaluation and comparison of different mining algorithms.
The best known generator for
transaction data for mining association rules
was developed by~\cite{arules:Agrawal+Srikant:1994}.
Alternatively data can be generated by simple probabilistic models 
as done by
\cite{arules:Hahsler+Hornik+Reutterer:2005}.

Finally, similarity measures between itemsets and rules can be
implemented for \pkg{arules}. With such measures distance based
clustering and visualization of associations is possible 
\citep[see e.g.,][]{arules:Strehl+Gosh:2003}).

\section*{Acknowledgments}
Part of \pkg{arules} was developed  during the project 
``Statistical Computing with R'' funded by  of the
``Jubil\"aumsstiftung der WU Wien.''
The authors of \pkg{arules} would like to thank Christian Borgelt for the
implementation of Apriori and Eclat.


\bibliographystyle{plainnat}
\bibliography{arules,arules_clustering}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
